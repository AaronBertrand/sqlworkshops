{
    "metadata": {
        "kernelspec": {
            "name": "SQL",
            "display_name": "SQL",
            "language": "sql"
        },
        "language_info": {
            "name": "sql",
            "version": ""
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": "<img src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/solutions-microsoft-logo-small.png?raw=true\" alt=\"Microsoft\">\r\n<br>\r\n\r\n# Workshop: Microsoft SQL Server Machine Learning Services\r\n\r\n#### <i>A Microsoft Course from the SQL Server team</i>\r\n\r\n## 06 - Phase 4: Deployment \r\n\r\n<p style=\"border-bottom: 1px solid lightgrey;\"></p>",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/textbubble.png?raw=true\"></p>\r\n\r\n<br>\r\n<br>\r\n<br>\r\n\r\nYou're learning to use the Team Data Science Process to create a complete solution, using SQL Server as the platform. The phases in the Team Data Science process are:\r\n\r\n<dl>\r\n  <dt>1 - Business Understanding</dt>\r\n  <dt>2 - Data Acquisition and Understanding</dt>\r\n  <dt>3 - Modeling</i></dt>\r\n  <dt>4 - Deployment <i>(This module)</dt>\r\n  <dt>5 - Customer Acceptance and Model Retraining</dt>\r\n<dl>\r\n\r\n<p style=\"border-bottom: 1px solid lightgrey;\"></p>\r\n\r\n<img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/pin.jpg?raw=true\">\r\n<br>\r\n\r\nIn this phase you'll take the Trained Model from the Azure environment and then use the Model in SQL Server using ML Services over hybrid data.\r\n\r\n### Goal for Deployment\r\n- Deploy models with a data pipeline to a production or production-like environment for final user acceptance\r\n\r\n### How to do it\r\n- Deploy the model and pipeline to a production or production-like environment for application consumption\r\n",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/pin.jpg?raw=true\"><b>4.1 Store the Model</b></p>\r\n\r\nIn this section, you will move the Trained Model you created in your Jupyter Notebook to SQL Server and deploy your predictive model with the help of SQL Server Machine Learning Services.\r\n\r\nTo deploy a model, you store the model in a hosting environment and implement a prediction function that uses the model to predict. That function can be called from applications, on a schedule, or an API service.\r\n\r\nNote that you could have trained and created the model directly in SQL Server ML Services using Python or R locally in the context of SQL Server. You can author T-SQL programs that contain embedded Python or R scripts, and the SQL Server database engine takes care of the execution. Because it executes in SQL Server, your training can easily be run against data stored in the database. In this scenario however, you wanted to explore a common process of working with a Jupyter Notebook locally.\r\n\r\nIn either case, to deploy the model so that you can use it in SQL Server, you will need to either copy the model to a location and SQL Server can read, load it into a binary column of a table in the SQL Server database, or train it in SQL Server. To use the model, you will create a stored procedure that does predictions by calling the Model.\r\n\r\nThe model is stored as a binary object in SQL Server, so you'll begin by making a SQL Server table to hold the binary object.\r\n\r\nIn this course, your model file is created using Python from a Jupyter Notebook. Whatever technology you use, the key to this \"Train elsewhere, use in SQL Server\" scenario is that you are able to create a binary representation of your Trained Model for use in SQL Server.\r\n\r\nIf you are transferring a trained binary model in, you'll use code to create the  binary file *(called a pickle in Python, or a stream in R)* that contains the Model. There are other serialization methods available, including ONNX that you can use to store your model. If you use Azure Machine Learning Services, the Model is stored in the Cloud.\r\n\r\n**NOTE: The Model essentially contains the code from the final training. That means that the receiving system (SQL Server in this course) must be able to run that code, have the same libraries and so on.**\r\n\r\nFor this course, the same Features (minus the Labels since this is a clustering exercise) have been stored in a table. This is data created directly from production, which the model has not seen yet. This has been done for you already, but in production you would work with the SQL Server team to create exactly the same transforms that were used to train the model in the local environment. You're going to re-train the model, again a process that is common in production.\r\n\r\nIf you create the training for the model on a local system, it's important to use as similar data (as much as possible) for the model predictions. The data you trained with needs to represent as closely as possible the type of data you're scoring, or you could get incorrect results. \r\n",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/checkbox.png?raw=true\"><b>Activity: Copy the Model File</b></p>\r\n\r\nIn this exercise, you will simulate downloading the Trained Model from Azure by creating a directory that SQL Server has permissions to and copying the latest trained model there.\r\n\r\n- From the File Explorer on your DSVM, open the `ML Services for SQL Server/notebooks/result_folder` folder.\r\n- Locate the last model (which you determined was the \"best\" one from the review stage in the Jupyter Notebook) - `model_2.pkl`.\r\n- Copy that file to the BACKUP directory in SQL Server - normally located at `C:\\Program Files\\Microsoft SQL Server\\MSSQL14.MSSQLSERVER\\MSSQL\\Backup`. If that location does not exist, you can create a directory anywhere you like, grant full permissions to the account running the SQL Server Service, and store the model there. If you do that, you will need to change the T-SQL statement that loads the data to that new location.\r\n\r\nYou'll use Visual Studio Code to run these T-SQL statements, but you can also use tools such as Visual Studio, SQL Server Management Studio, or the Azure Data Studio as well. Remain in Visual Studio Code throughout this section.\r\n",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/checkbox.png?raw=true\"><b>Activity: Create a table for the binary Model</b></p>\r\n\r\n- Open Visual Studio Code, and then open the course file `ML Services for SQL Server/code/SQL Scripts for Hybrid ML Course.sql`.\r\n\r\n- Press F1 and type `sql connect` to connect to your SQL Server Instance. <a href=\"https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-develop-use-vscode?view=sql-server-2017\" target=_blank>(More on how to do that here)</a>:\r\n\r\n- Find the section in the file marked `/* Activity: Import Model */`, and run the code (highlight and press CTRL-SHIFT-E) from that location until `/* End Activity: Import Model */`. Follow any instructions you see there.\r\n",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/pin.jpg?raw=true\"><b>5.2 Run Predictions Over Production Data</b></p>\r\n\r\nEverything is now in place for you to create the Stored Procedure that will wait for data as input and then run the model, predicting the outcome. You'll send in the Features (in this case, using a SELECT statement as the input variable).\r\n",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/checkbox.png?raw=true\"><b>Activity: Generate Stored Procedures for Predictions</b></p>\r\n\r\n- Find the section in the .sql file marked `/* Activity: Generate Stored Procedure for predictions */`, and run the code (highlight and press CTRL-SHIFT-E) from that location until `/* End Activity: Generate Stored Procedure for predictions */`. Follow any instructions you see there.\r\n- ",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/pin.jpg?raw=true\"><b>5.3 View Predictions</b></p>\r\n\r\nFinally, you're ready to send in new data that the model has *not* seen, and make predictions for which components are likely to fail and need to be addressed.\r\n\r\nIn this exercise you're simply going to call the Stored Procedure you just created, but in production, you could also persist the data it returns into a table that you can examine with Power BI or some other Visualization tool.\r\n",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/checkbox.png?raw=true\"><b>Activity: View Predictions</b></p>\r\n\r\n- Find the section in the .sql file marked `/* Activity: Return Results */`, and run the code (highlight and press CTRL-SHIFT-E) from that location until `/* End Activity: Return Results */`. Follow any instructions you see there.\r\n- *Optional*: Change these statements to store the data with a date-stamp column in another table. Create a visualization of this new table.\r\n- *Optional*: Create a Power BI report that views the data stored in the table stored in the previous step, or have it directly run the prediction and create a real-time chart of the predictions.\r\n",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/thinking.jpg?raw=true\"><b>For Further Study</b></p>\r\n\r\n<br>\r\n<br>\r\n\r\n - Another example of using a Stored Procedure to predict data using R and SQL Server: https://docs.microsoft.com/en-us/sql/advanced-analytics/tutorials/rtsql-create-a-predictive-model-r?view=sql-server-2017\r\n",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/education1.png?raw=true\"><b>Next</b>: Phase 5 - Customer Acceptance and Model Retraining</p>\r\n\r\nNext, you'll start working through the Team Data Science Process in the next phase - Customer Acceptance and Model Retraining. Open that Notebook to continue.",
            "metadata": {}
        }
    ]
}