{
    "metadata": {
        "kernelspec": {
            "name": "pyspark3kernel",
            "display_name": "PySpark3"
        },
        "language_info": {
            "name": "python",
            "version": ""
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": "<img src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/solutions-microsoft-logo-small.png?raw=true\" alt=\"Microsoft\">\r\n<br>\r\n\r\n# SQL Server 2019 big data cluster Tutorial\r\n## 05 - Using Spark For Machine Learning\r\n\r\nIn this tutorial you will learn how to work with Spark Jobs in a SQL Server big data cluster. \r\n\r\n**TODO:** Complete Tutorial ",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# https://github.com/solliancenet/tech-immersion-data-ai/blob/master/day1-exp1/notebook.pdf \r\nimport pickle \r\nimport pandas as pd\r\nimport numpy as np\r\nimport datetime as dt\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.model_selection import train_test_split",
            "metadata": {},
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Starting Spark application\n"
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1556023647622_0002</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"https://10.84.56.16:30443/gateway/default/yarn/proxy/application_1556023647622_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://mssql-storage-pool-default-1.service-storage-pool-default.sqlbigdata.svc.cluster.local:8042/node/containerlogs/container_1556023647622_0002_01_000001/root\">Link</a></td><td>âœ”</td></tr></table>"
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "SparkSession available as 'spark'.\n"
                }
            ],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": "df = pd.read_csv('https://raw.githubusercontent.com/solliancenet/tech-immersion-data-ai/master/environment-setup/data/1/training-formatted.csv', header=0)\r\ndf.dropna()\r\nprint(df.shape)\r\nprint(list(df.columns))",
            "metadata": {},
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "(10000, 74)\n['Survival_In_Days', 'Province', 'Region', 'Trip_Length_Mean', 'Trip_Length_Sigma', 'Trips_Per_Day_Mean', 'Trips_Per_Day_Sigma', 'Battery_Rated_Cycles', 'Manufacture_Month', 'Manufacture_Year', 'Alternator_Efficiency', 'Car_Has_EcoStart', 'Twelve_hourly_temperature_history_for_last_31_days_before_death_last_recording_first', 'Sensor_Reading_1', 'Sensor_Reading_2', 'Sensor_Reading_3', 'Sensor_Reading_4', 'Sensor_Reading_5', 'Sensor_Reading_6', 'Sensor_Reading_7', 'Sensor_Reading_8', 'Sensor_Reading_9', 'Sensor_Reading_10', 'Sensor_Reading_11', 'Sensor_Reading_12', 'Sensor_Reading_13', 'Sensor_Reading_14', 'Sensor_Reading_15', 'Sensor_Reading_16', 'Sensor_Reading_17', 'Sensor_Reading_18', 'Sensor_Reading_19', 'Sensor_Reading_20', 'Sensor_Reading_21', 'Sensor_Reading_22', 'Sensor_Reading_23', 'Sensor_Reading_24', 'Sensor_Reading_25', 'Sensor_Reading_26', 'Sensor_Reading_27', 'Sensor_Reading_28', 'Sensor_Reading_29', 'Sensor_Reading_30', 'Sensor_Reading_31', 'Sensor_Reading_32', 'Sensor_Reading_33', 'Sensor_Reading_34', 'Sensor_Reading_35', 'Sensor_Reading_36', 'Sensor_Reading_37', 'Sensor_Reading_38', 'Sensor_Reading_39', 'Sensor_Reading_40', 'Sensor_Reading_41', 'Sensor_Reading_42', 'Sensor_Reading_43', 'Sensor_Reading_44', 'Sensor_Reading_45', 'Sensor_Reading_46', 'Sensor_Reading_47', 'Sensor_Reading_48', 'Sensor_Reading_49', 'Sensor_Reading_50', 'Sensor_Reading_51', 'Sensor_Reading_52', 'Sensor_Reading_53', 'Sensor_Reading_54', 'Sensor_Reading_55', 'Sensor_Reading_56', 'Sensor_Reading_57', 'Sensor_Reading_58', 'Sensor_Reading_59', 'Sensor_Reading_60', 'Sensor_Reading_61']"
                }
            ],
            "execution_count": 4
        },
        {
            "cell_type": "code",
            "source": "# Select the features used for predicting battery life\r\nx = df.iloc[:,1:74]\r\nx = x.iloc[:,np.r_[2:7, 9:73]]\r\nx = x.interpolate() \r\n\r\n# Select the labels only (the measured battery life) \r\ny = df.iloc[:,0].values.flatten()",
            "metadata": {},
            "outputs": [],
            "execution_count": 5
        },
        {
            "cell_type": "code",
            "source": "# Examine the features selected \r\nprint(list(x.columns))",
            "metadata": {},
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "['Trip_Length_Mean', 'Trip_Length_Sigma', 'Trips_Per_Day_Mean', 'Trips_Per_Day_Sigma', 'Battery_Rated_Cycles', 'Alternator_Efficiency', 'Car_Has_EcoStart', 'Twelve_hourly_temperature_history_for_last_31_days_before_death_last_recording_first', 'Sensor_Reading_1', 'Sensor_Reading_2', 'Sensor_Reading_3', 'Sensor_Reading_4', 'Sensor_Reading_5', 'Sensor_Reading_6', 'Sensor_Reading_7', 'Sensor_Reading_8', 'Sensor_Reading_9', 'Sensor_Reading_10', 'Sensor_Reading_11', 'Sensor_Reading_12', 'Sensor_Reading_13', 'Sensor_Reading_14', 'Sensor_Reading_15', 'Sensor_Reading_16', 'Sensor_Reading_17', 'Sensor_Reading_18', 'Sensor_Reading_19', 'Sensor_Reading_20', 'Sensor_Reading_21', 'Sensor_Reading_22', 'Sensor_Reading_23', 'Sensor_Reading_24', 'Sensor_Reading_25', 'Sensor_Reading_26', 'Sensor_Reading_27', 'Sensor_Reading_28', 'Sensor_Reading_29', 'Sensor_Reading_30', 'Sensor_Reading_31', 'Sensor_Reading_32', 'Sensor_Reading_33', 'Sensor_Reading_34', 'Sensor_Reading_35', 'Sensor_Reading_36', 'Sensor_Reading_37', 'Sensor_Reading_38', 'Sensor_Reading_39', 'Sensor_Reading_40', 'Sensor_Reading_41', 'Sensor_Reading_42', 'Sensor_Reading_43', 'Sensor_Reading_44', 'Sensor_Reading_45', 'Sensor_Reading_46', 'Sensor_Reading_47', 'Sensor_Reading_48', 'Sensor_Reading_49', 'Sensor_Reading_50', 'Sensor_Reading_51', 'Sensor_Reading_52', 'Sensor_Reading_53', 'Sensor_Reading_54', 'Sensor_Reading_55', 'Sensor_Reading_56', 'Sensor_Reading_57', 'Sensor_Reading_58', 'Sensor_Reading_59', 'Sensor_Reading_60', 'Sensor_Reading_61']"
                }
            ],
            "execution_count": 6
        },
        {
            "cell_type": "code",
            "source": "# Train a regression model \r\nfrom sklearn.ensemble import GradientBoostingRegressor \r\nmodel = GradientBoostingRegressor() \r\nmodel.fit(x,y)",
            "metadata": {},
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n             max_leaf_nodes=None, min_impurity_decrease=0.0,\n             min_impurity_split=None, min_samples_leaf=1,\n             min_samples_split=2, min_weight_fraction_leaf=0.0,\n             n_estimators=100, n_iter_no_change=None, presort='auto',\n             random_state=None, subsample=1.0, tol=0.0001,\n             validation_fraction=0.1, verbose=0, warm_start=False)"
                }
            ],
            "execution_count": 7
        },
        {
            "cell_type": "code",
            "source": "# Try making a single prediction and observe the result \r\nmodel.predict(x.iloc[0:1]) ",
            "metadata": {},
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "array([1323.39791998])"
                }
            ],
            "execution_count": 8
        },
        {
            "cell_type": "code",
            "source": "# access the test data from HDFS by reading into a Spark DataFrame \r\ntest_data = pd.read_csv('https://raw.githubusercontent.com/solliancenet/tech-immersion-data-ai/master/environment-setup/data/1/fleet-formatted.csv', header=0)\r\ntest_data.dropna()\r\n\r\n# prepare the test data (dropping unused columns) \r\ntest_data = test_data.drop(columns=[\"Car_ID\", \"Battery_Age\"])\r\ntest_data = test_data.iloc[:,np.r_[2:7, 9:73]]\r\ntest_data.rename(columns={'Twelve_hourly_temperature_forecast_for_next_31_days _reversed': 'Twelve_hourly_temperature_history_for_last_31_days_before_death_l ast_recording_first'}, inplace=True) \r\n# make the battery life predictions for each of the vehicles in the test data \r\nbattery_life_predictions = model.predict(test_data) \r\n# examine the prediction \r\nbattery_life_predictions",
            "metadata": {},
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "array([1472.91111228, 1340.08897725, 1421.38601032, 1473.79033215,\n       1651.66584142, 1412.85617044, 1842.81351408, 1264.22762055,\n       1930.45602533, 1681.86345995])"
                }
            ],
            "execution_count": 9
        },
        {
            "cell_type": "code",
            "source": "# prepare one data frame that includes predictions for each vehicle \r\nscored_data = test_data \r\nscored_data[\"Estimated_Battery_Life\"] = battery_life_predictions \r\ndf_scored = spark.createDataFrame(scored_data) \r\n# Write out the scored data: \r\ndf_scored.coalesce(1).write.option(\"header\", \"true\").csv(\"/pdm\") ",
            "metadata": {},
            "outputs": [],
            "execution_count": 13
        },
        {
            "cell_type": "code",
            "source": "pickle_file = open('/tmp/pdm.pkl', 'wb')\r\npickle.dump(model, pickle_file)\r\nimport os\r\nprint(os.getcwd())\r\nos.listdir('///tmp')",
            "metadata": {},
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "/tmp/nm-local-dir/usercache/root/appcache/application_1555723257235_0003/container_1555723257235_0003_01_000001\n['test.pkl', 'pdm.pkl', 'Jetty_0_0_0_0_8042_node____19tj0x', 'nm-local-dir', 'Jetty_localhost_44079_datanode____j8a1yx', 'test.txt', 'tmppboajmm2', 'hsperfdata_root', 'install.sh']"
                }
            ],
            "execution_count": 14
        },
        {
            "cell_type": "markdown",
            "source": "## Next Steps: Continue on to other workloads in SQL Server 2019\r\n\r\nNow you're ready to work with SQL Server 2019's other features - [you can learn more about those here](https://docs.microsoft.com/en-us/sql/big-data-cluster/big-data-cluster-overview?view=sqlallproducts-allversions).",
            "metadata": {}
        }
    ]
}